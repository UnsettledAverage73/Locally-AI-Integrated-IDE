# LocalDev - Privacy-First, Offline-Capable AI IDE

**Tagline:** The Privacy-First, Offline-Capable AI IDE.

## ğŸ¯ Overview

LocalDev is an Integrated Development Environment (IDE) that runs Large Language Models (LLMs) entirely on your local machine. It indexes your codebase locally using a vector database to provide context-aware AI assistance without a single byte of data leaving your laptop.

## âœ¨ Features

- **ğŸ”’ Privacy-First**: Zero telemetry. No API calls to cloud services.
- **ğŸ’» Local AI**: Runs entirely on your machine using Ollama
- **ğŸ§  RAG Pipeline**: Intelligent code indexing with LanceDB
- **ğŸ’¬ Chat Interface**: Context-aware AI assistant
- **ğŸ“ Code Editor**: Monaco Editor with syntax highlighting
- **ğŸ“ File Explorer**: Navigate your project files
- **âš™ï¸ Model Switcher**: Switch between different local LLM models
- **ğŸ–¥ï¸ Terminal**: Built-in terminal for running commands

## ğŸš€ Prerequisites

1. **Node.js** (v18 or higher)
2. **Ollama** - [Install Ollama](https://ollama.ai/)
3. **At least one LLM model** installed in Ollama:
   ```bash
   ollama pull deepseek-coder
   ollama pull nomic-embed-text  # For embeddings
   ```

## ğŸ“¦ Installation

1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd Local-AI-IDE
   ```

2. Install dependencies:
   ```bash
   npm install
   ```

3. Start Ollama service (if not already running):
   ```bash
   ollama serve
   ```

## ğŸƒ Running the Application

### Development Mode

```bash
npm run electron:dev
```

This will:
- Start the Vite dev server
- Launch Electron with hot reload

### Build for Production

```bash
npm run build
```

This creates distributable packages in the `release/` directory.

## ğŸ› ï¸ Technology Stack

- **Framework**: Electron + React + TypeScript
- **Editor**: Monaco Editor (@monaco-editor/react)
- **AI Engine**: Ollama (local inference)
- **Database**: LanceDB (vector database for RAG)
- **Styling**: Tailwind CSS
- **Build Tool**: Vite

## ğŸ“‹ Functional Requirements

| ID | Feature | Status |
|----|---------|--------|
| FR-01 | Editor Core | âœ… Complete |
| FR-02 | Local LLM Integration | âœ… Complete |
| FR-03 | RAG Pipeline (Memory) | âœ… Complete |
| FR-04 | Chat Interface | âœ… Complete |
| FR-05 | Model Switcher | âœ… Complete |
| FR-06 | Terminal Integration | âœ… Complete |

## ğŸ¨ Usage

1. **Open a Project**: Click "Open Folder" to select your project directory
2. **Wait for Indexing**: The IDE will automatically index your codebase
3. **Ask Questions**: Use the chat panel to ask about your code
4. **Switch Models**: Go to Settings to change the AI model
5. **Edit Code**: Use the Monaco editor with full syntax highlighting

## ğŸ”§ Configuration

### Changing AI Models

1. Open Settings (âš™ï¸ icon in top bar)
2. Select your preferred model from the dropdown
3. Make sure the model is installed in Ollama:
   ```bash
   ollama pull <model-name>
   ```

### Recommended Models

- **For Coding**: `deepseek-coder`, `codellama`, `starcoder`
- **For Chat**: `llama3`, `mistral`, `phi`
- **For Embeddings**: `nomic-embed-text`

## ğŸ—ï¸ Project Structure

```
Local-AI-IDE/
â”œâ”€â”€ electron/          # Electron main process
â”‚   â”œâ”€â”€ main.ts        # Main process entry
â”‚   â””â”€â”€ preload.ts     # Preload script
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/    # React components
â”‚   â”‚   â”œâ”€â”€ Editor.tsx
â”‚   â”‚   â”œâ”€â”€ ChatPanel.tsx
â”‚   â”‚   â”œâ”€â”€ FileExplorer.tsx
â”‚   â”‚   â”œâ”€â”€ Terminal.tsx
â”‚   â”‚   â””â”€â”€ Settings.tsx
â”‚   â”œâ”€â”€ services/      # Business logic
â”‚   â”‚   â”œâ”€â”€ ollama.ts  # Ollama integration
â”‚   â”‚   â””â”€â”€ rag.ts     # RAG pipeline
â”‚   â”œâ”€â”€ App.tsx        # Main app component
â”‚   â””â”€â”€ main.tsx       # React entry point
â””â”€â”€ package.json
```

## ğŸ› Troubleshooting

### Ollama Connection Issues

If you see "Ollama backend is not reachable" or connection errors:
1. Make sure Ollama is installed and running: `ollama serve`
2. Make sure the LocalDev backend is running (it should start automatically with `npm run dev`).
3. Check the backend terminal for any errors related to Ollama or LanceDB.
4. Verify Ollama is accessible directly: `curl http://localhost:11434/api/tags`

### Model Not Found

If a model isn't available:
1. Install it via Ollama: `ollama pull <model-name>`
2. Refresh models in LocalDev's Settings.

### Indexing Issues

If code indexing fails:
1. Ensure Ollama's `nomic-embed-text` model is pulled: `ollama pull nomic-embed-text`
2. Check the backend terminal for RAG or LanceDB errors.
3. Verify the project folder is accessible by the backend process.

### Frontend Build Errors
- Run `npm install` in the project root to install all workspace dependencies.
- Then run `npm run build:frontend` to build the frontend specifically.

## ğŸ”’ Privacy & Security

- **Zero Telemetry**: No data is sent to external servers
- **Local Processing**: All AI inference happens on your machine
- **No Cloud Calls**: No API calls to OpenAI, Anthropic, or any cloud service
- **Offline Capable**: Works completely offline after initial setup

## ğŸ“ License

MIT License

## ğŸ‘¥ Team

- Lead Developer: Backend & AI Engine
- Frontend Specialist: UI/UX & Editor
- System & Testing: File System & DevOps

## ğŸ—ºï¸ Roadmap

- [ ] Multi-file editing with tabs
- [ ] Git integration
- [ ] Code completion (autocomplete)
- [ ] Debugger integration
- [ ] Plugin system
- [ ] Custom themes
- [ ] Performance optimizations

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

**Built with â¤ï¸ for developers who value privacy and offline capability.**

